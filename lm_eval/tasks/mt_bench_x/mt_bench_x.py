# TODO: Remove all TODO comments once the implementation is complete.
"""
TODO: Add the Paper Title on this line.
TODO: Add the paper's PDF URL (preferably from arXiv) on this line.

TODO: Write a Short Description of the task.

Homepage: TODO: Add the URL to the task's Homepage here.
"""
from dataclasses import dataclass
from typing import Dict
import fastchat
from fastchat.conversation import Conversation, SeparatorStyle
import warnings
from lm_eval.base import Task, rf


# TODO: Add the BibTeX citation for the task.
_CITATION = """
"""


# TODO: Replace `NewTask` with the name of your Task.
class MTBenchX(Task):
    VERSION = 0
    # TODO: Add the `DATASET_PATH` string. This will be the name of the `Task`
    # dataset as denoted in HuggingFace `datasets`.
    DATASET_PATH = ""
    # TODO: Add the `DATASET_NAME` string. This is the name of a subset within
    # `DATASET_PATH`. If there aren't specific subsets you need, leave this as `None`.
    DATASET_NAME = None

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        if self.has_training_docs():
            # We cache training documents in `self._training_docs` for faster
            # few-shot processing. If the data is too large to fit in memory,
            # return the training data as a generator instead of a list.
            if self._training_docs is None:
                self._training_docs = list(self.dataset["train"])

            # doc_keys = ["question_id", "category", "turns", "lang", ["judge_prompt_single-v1", ...], ]
            return self._training_docs

    def validation_docs(self):
        if self.has_validation_docs():
            return self.dataset["validation"]

    def test_docs(self):
        if self.has_test_docs():
            return self.dataset["test"]

    def _process_doc(self, doc):
        # TODO: Process (detokenize, strip, replace etc.) each individual `doc`
        # with this function. You can map this across the docs in each available
        # dataset split. See the TODOs in `train_docs`, `validation_docs`, and
        # `test_docs` for snippets.
        # NOTE: DELETE THIS FUNCTION IF UNUSED.
        return doc

    def doc_to_text(self, doc):
        # with instruction-tuned models we do not support few-shot examples, i.e. no meaningful ctx is created
        return ""

    def doc_to_target(self, doc):
        # with instruction-tuned models we do not support few-shot examples, i.e. no meaningful ctx is created
        target = ""
        return " " + target

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or
            test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        warnings.warn(
            "MT-Bench-X is not yet fully supported! All categories are decoded by greedy sampling."
            "No temperature adaptions on the basis of categories are made yet!"
            "Only OpenGPT-X chat template supported by now!",
        )
        lang_code = doc["lang"]
        conv = get_conv_template()
        conv.select_system_message_by_language(lang_code=lang_code)
        for j in range(len(doc["turns"])):
            qs = doc["turns"][j]
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], None)
            prompt = conv.get_prompt()
            
            stop = [conv.stop_str]
            if conv.sep2 is not None:
                stop += [conv.sep2] 
            # TODO: include gen_kwargs, once we migrate to the newest lm-eval-harness version!
            yield rf.greedy_until(prompt, {"until": stop})

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        # TODO: For each (sub)metric in the task evaluation, add a key-value pair
        # with the metric name as key and the corresponding metric result as value
        # for the current `doc`.

        judges = make_judge_single(args.judge_model, judge_prompts)
        play_a_match_func = play_a_match_single
        output_file = f"data/{args.bench_name}/model_judgment/{args.judge_model}_single.jsonl"
        make_match_func = make_match_single
        return {f"mt-bench-{doc['lang']}": value}

    def aggregation(self):
        """
        :returns: {str: [metric_score] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metric scores
        """
        # TODO: For each (sub)metric in the task evaluation, add a key-value pair
        # with the metric name as key and an aggregation function as value which
        # determines how to combine results from each document in the dataset.
        # Check `lm_eval.metrics` to find built-in aggregation functions.
        return {}

    def higher_is_better(self):
        # TODO: For each (sub)metric in the task evaluation, add a key-value pair
        # with the metric name as key and a `bool` value determining whether or
        # not higher values of that metric are deemed better.
        return {}

@dataclass
class MultilingualConversation(Conversation):
    system_messages: Dict[str, str] ={}

    def select_system_message_by_language(self, lang_code: str):
        self.system_message = self.system_messages[lang_code]

def get_conv_template():
    return MultilingualConversation(
        name="opengptx",
        system_template="System: {system_message}",
        system_message="A chat between a human and an artificial intelligence assistant. "
        "The assistant gives helpful and polite answers to the human's questions.",
        roles=("User", "Assistant"),
        sep_style=SeparatorStyle.COLON_SURROUND,
        sep="<s>",
        sep2="</s>",
        system_messages={
            "EN": "A chat between a human and an artificial intelligence assistant."
            " The assistant gives helpful and polite answers to the human's questions.",
            "DE": "Ein Gespräch zwischen einem Menschen und einem Assistenten mit künstlicher Intelligenz."
            " Der Assistent gibt hilfreiche und höfliche Antworten auf die Fragen des Menschen.",
            "FR": "Conversation entre un humain et un assistant doté d'une intelligence artificielle."
            " L'assistant donne des réponses utiles et polies aux questions de l'homme.",
            "IT": "Una chat tra un umano e un assistente di intelligenza artificiale."
            " L'assistente fornisce risposte utili ed educate alle domande dell'uomo.",
            "ES": "Una conversación entre un humano y un asistente de inteligencia artificial."
            " El asistente da respuestas útiles y amables a las preguntas del humano.",
        },
        eos_token="<eod>",
        stop_str="</s>",
    )