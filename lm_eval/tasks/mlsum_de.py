import datasets
from lm_eval.base import rf, Task
from functools import partial


def _mlsum_metric(predictions, references):
    summarization_metric = datasets.load_metric("rouge")
    return summarization_metric.compute(
        predictions=predictions, references=references, use_agregator=True
    )


def _mlsum_agg(key, items):
    predictions, references = zip(*items)
    return _mlsum_metric(predictions=predictions, references=references)[
        key
    ].mid.fmeasure


class MLSUM(Task):
    VERSION = None
    DATASET_PATH = "mlsum"
    DATASET_NAME = "de"

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.data["train"]

    def validation_docs(self):
        return self.data["validation"]

    def test_docs(self):
        return self.dataset["test"]

    # def fewshot_description(self):
    #     return "Summarize the following articles."

    def doc_to_text(self, doc):
        return "Artikel: " + doc["text"] + "\n\n" + "Zusammenfassung:"

    def doc_to_target(self, doc):
        summary = doc["summary"]
        return " " + summary

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        continuation = rf.greedy_until(ctx, ["\n"], True, 2, 100)
        #  arg3: do sampling, arg4: top k, arg5: max gen tokens
        #  arg4 and 5 are taken from gpt2 paper
        return continuation

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        continuation = results

        predictions = {
            "id": doc["topic"],
            "prediction_text": continuation,
            "no_answer_probability": 0,
        }

        references = {
            "id": doc["topic"],
            "answers": [doc["summary"]],
        }

        return {
            "rouge1": (predictions, references),
            "rouge2": (predictions, references),
            "rougeL": (predictions, references),
        }

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {
            "rouge1": partial(_mlsum_agg, "rouge1"),
            "rouge2": partial(_mlsum_agg, "rouge2"),
            "rougeL": partial(_mlsum_agg, "rougeL"),
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {"rouge1": True, "rouge2": True, "rougeL": True}
